{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports\n",
    "import gradio as grad\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading and inferencing for Promptist</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompter():\n",
    "  prompter_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Promptist\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  tokenizer.padding_side = \"left\"\n",
    "  return prompter_model, tokenizer\n",
    "\n",
    "prompter_model, prompter_tokenizer = load_prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(plain_text):\n",
    "    input_ids = prompter_tokenizer(plain_text.strip()+\" Rephrase:\", return_tensors=\"pt\").input_ids\n",
    "    eos_id = prompter_tokenizer.eos_token_id\n",
    "    outputs = prompter_model.generate(input_ids, do_sample=False, max_new_tokens=75, num_beams=8, num_return_sequences=8, eos_token_id=eos_id, pad_token_id=eos_id, length_penalty=-1.0)\n",
    "    output_texts = prompter_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    res = output_texts[0].replace(plain_text+\" Rephrase:\", \"\").strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = grad.Textbox(lines=1, label=\"Initial Text\", placeholder=\"Input Prompt\")\n",
    "out = grad.Textbox(lines=1, label=\"Optimized Prompt\")\n",
    "examples = [\"A rabbit is wearing a space suit\", \"Several railroad tracks with one train passing by\", \"The roof is wet from the rain\", \"Cats dancing in a space club\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.Interface(fn=generate,\n",
    "               inputs=txt,\n",
    "               outputs=out,\n",
    "               title=\"Promptist Demo\",\n",
    "               description=\"Promptist is a prompt interface for Stable Diffusion v1-4 (https://huggingface.co/CompVis/stable-diffusion-v1-4) that optimizes user input into model-preferred prompts.\",\n",
    "               examples=examples,\n",
    "               allow_flagging='never',\n",
    "               cache_examples=False,\n",
    "               theme=\"default\").launch(enable_queue=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading and inferencing for Stable Diffusion</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]  \n",
    "    \n",
    "image.save(\"astronaut_rides_horse.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
